{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJSONObj(filename):\n",
    "    with open(filename) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening original SQuAD 2.0 train and dev data\n",
    "squad_train = getJSONObj('original_squad_dataset/train-v2.0.json')\n",
    "squad_dev = getJSONObj('original_squad_dataset/dev-v2.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining SQuAD 2.0 train and dev data and then shuffling\n",
    "squad_train_dev_combined = {\"version\": \"v2.0\"}\n",
    "squad_train_dev_combined['data'] = squad_train['data'] + squad_dev['data']\n",
    "random.shuffle(squad_train_dev_combined['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "###### APPLYING BASELINE 1 MODIFICATIONS - REMOVING NON-CONTEXT WORDS ######\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import copy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isStopWord(token):\n",
    "    return token.is_stop\n",
    "\n",
    "def isInContext(token, context_tokens):\n",
    "    return token.text.lower() in context_tokens\n",
    "\n",
    "def isPunctuation(token):\n",
    "    return token.text in string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_train_dev_combined = copy.deepcopy(squad_train_dev_combined)\n",
    "\n",
    "flag = False\n",
    "for topic in b1_train_dev_combined['data']:\n",
    "    # if flag:\n",
    "    #     break\n",
    "    for paragraph in topic['paragraphs']:\n",
    "        # if flag:\n",
    "        #     break\n",
    "        new_questions = []\n",
    "        context = paragraph['context']\n",
    "        context_tokens = [t.text.lower() for t in nlp(context)]\n",
    "        i = 0\n",
    "        while i < len(paragraph['qas']):\n",
    "            question = qa['question']\n",
    "            question_doc = nlp(question)\n",
    "            new_question_tokens = []\n",
    "            for token in question_doc:\n",
    "                if isPunctuation(token) or isStopWord(token) or isInContext(token, context_tokens):\n",
    "                    new_question_tokens.append(token.text)\n",
    "            new_question = TreebankWordDetokenizer().detokenize(new_question_tokens)\n",
    "            if new_question == \"\":\n",
    "                paragraph['qas'].pop(i)\n",
    "            else:\n",
    "                qa['question'] = new_question\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "###### APPLYING BASELINE 2 MODIFICATIONS - REPLACING NON-CONTEXT WORDS WITH CLOSEST FASTTEXT EMBEDDING IN CONTEXT ######\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "model = FastText.load_fasttext_format('wiki.simple')\n",
    "print(\"Model Loaded...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostSimContextWord(token, context_tokens):\n",
    "    highest_sim = float('-Inf')\n",
    "    most_sim_word = None\n",
    "    for ct in context_tokens:\n",
    "        if ct.lower() in model.wv.vocab:\n",
    "            curr_sim = model.similarity(token.text.lower(), ct.lower())\n",
    "            if curr_sim >= highest_sim:\n",
    "                highest_sim = curr_sim\n",
    "                most_sim_word = ct\n",
    "    return most_sim_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_train_dev_combined = copy.deepcopy(squad_train_dev_combined)\n",
    "\n",
    "flag = False\n",
    "count = 0\n",
    "datalen = len(b2_train_dev_combined['data'])\n",
    "for topic in b2_train_dev_combined['data']:\n",
    "    # if flag:\n",
    "    #     break\n",
    "    for paragraph in topic['paragraphs']:\n",
    "        # if flag:\n",
    "        #     break\n",
    "        context = paragraph['context']\n",
    "        context_tokens = [t for t in nlp(context) if not isPunctuation(t) and not isStopWord(t)]\n",
    "        context_token_text = [t.text for t in context_tokens]\n",
    "        context_token_lemmas = [t.lemma_.lower() for t in context_tokens]\n",
    "        i = 0\n",
    "        for qa in paragraph['qas']:\n",
    "            # if i > 5:\n",
    "            #     flag = True\n",
    "            question = qa['question']\n",
    "            question_doc = nlp(question)\n",
    "            new_question_tokens = []\n",
    "            for token in question_doc:\n",
    "                if isPunctuation(token) or isStopWord(token) or isInContext(token, context_token_lemmas):\n",
    "                    new_question_tokens.append(token.text)\n",
    "                else:\n",
    "                    if token.text.lower() in model.wv.vocab:\n",
    "                        word = getMostSimContextWord(token, context_token_text)\n",
    "                    else:\n",
    "                        word = token.text\n",
    "                    new_question_tokens.append(word)\n",
    "            new_question = TreebankWordDetokenizer().detokenize(new_question_tokens)\n",
    "            qa['question'] = new_question\n",
    "            i += 1\n",
    "    count += 1\n",
    "    print(\"{}/{} topics done\".format(count, datalen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "####################### APPLYING 70-20-10 SPLIT TO COMBINED DATASETS TO GET TRAIN, DEV, TEST SETS ######################\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original SQuAD dataset\n",
    "total_n = len(squad_train_dev_combined['data'])\n",
    "\n",
    "train_n = int(.7 * total_n)\n",
    "dev_n = int(.2 * total_n)\n",
    "\n",
    "modified_squad_train = {'version':'v2.0'}\n",
    "modified_squad_train['data'] = squad_train_dev_combined['data'][:train_n]\n",
    "\n",
    "modified_squad_dev = {'version':'v2.0'}\n",
    "modified_squad_dev['data'] = squad_train_dev_combined['data'][train_n: train_n+dev_n]\n",
    "\n",
    "modified_squad_test = {'version': 'v2.0'}\n",
    "modified_squad_test['data'] = squad_train_dev_combined['data'][train_n+dev_n: ]\n",
    "\n",
    "with open('modified_squad_dataset/modified_squad_train.json', 'w') as fp:\n",
    "    json.dump(modified_train, fp)\n",
    "\n",
    "with open('modified_squad_dataset/modified_squad_dev.json', 'w') as fp:\n",
    "    json.dump(modified_dev, fp)\n",
    "\n",
    "with open('modified_squad_dataset/modified_squad_test.json', 'w') as fp:\n",
    "    json.dump(modified_test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_n = len(b1_train_dev_combined['data'])\n",
    "\n",
    "train_n = int(.7 * total_n)\n",
    "dev_n = int(.2 * total_n)\n",
    "\n",
    "b1_train = {'version':'v2.0'}\n",
    "b1_train['data'] = b1_train_dev_combined['data'][:train_n]\n",
    "\n",
    "b1_dev = {'version':'v2.0'}\n",
    "b1_dev['data'] = b1_train_dev_combined['data'][train_n: train_n+dev_n]\n",
    "\n",
    "b1_test = {'version': 'v2.0'}\n",
    "b1_test['data'] = b1_train_dev_combined['data'][train_n+dev_n: ]\n",
    "\n",
    "with open('baseline1/b1_train.json', 'w') as fp:\n",
    "    json.dump(b1_train, fp)\n",
    "\n",
    "with open('baseline1/b1_dev.json', 'w') as fp:\n",
    "    json.dump(b1_dev, fp)\n",
    "\n",
    "with open('baseline1/b1_test.json', 'w') as fp:\n",
    "    json.dump(b1_test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_n = len(b2_train_dev_combined['data'])\n",
    "\n",
    "train_n = int(.7 * total_n)\n",
    "dev_n = int(.2 * total_n)\n",
    "\n",
    "b2_train = {'version':'v2.0'}\n",
    "b2_train['data'] = b2_train_dev_combined['data'][:train_n]\n",
    "\n",
    "b2_dev = {'version':'v2.0'}\n",
    "b2_dev['data'] = b2_train_dev_combined['data'][train_n: train_n+dev_n]\n",
    "\n",
    "b2_test = {'version': 'v2.0'}\n",
    "b2_test['data'] = b2_train_dev_combined['data'][train_n+dev_n: ]\n",
    "\n",
    "with open('baseline2/b2_train.json', 'w') as fp:\n",
    "    json.dump(b2_train, fp)\n",
    "\n",
    "with open('baseline2/b2_dev.json', 'w') as fp:\n",
    "    json.dump(b2_dev, fp)\n",
    "\n",
    "with open('baseline2/b2_test.json', 'w') as fp:\n",
    "    json.dump(b2_test, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
